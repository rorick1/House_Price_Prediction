{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Price_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "pRK9elAqrHpC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHE8tDQeYDQZ"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data set"
      ],
      "metadata": {
        "id": "UDyqUDOFrNk8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqQ8PijyYPvq"
      },
      "source": [
        "df=pd.read_csv('kc_house_data.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "AMwo3H7WYkFm",
        "outputId": "bff11088-b78b-4615-e3ef-081e9e0e4106"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-07045bdd-8583-48e5-8def-32e07e80edca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7129300520</td>\n",
              "      <td>10/13/2014</td>\n",
              "      <td>221900.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1180</td>\n",
              "      <td>5650</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1180</td>\n",
              "      <td>0</td>\n",
              "      <td>1955</td>\n",
              "      <td>0</td>\n",
              "      <td>98178</td>\n",
              "      <td>47.5112</td>\n",
              "      <td>-122.257</td>\n",
              "      <td>1340</td>\n",
              "      <td>5650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6414100192</td>\n",
              "      <td>12/9/2014</td>\n",
              "      <td>538000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2570</td>\n",
              "      <td>7242</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2170</td>\n",
              "      <td>400</td>\n",
              "      <td>1951</td>\n",
              "      <td>1991</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7210</td>\n",
              "      <td>-122.319</td>\n",
              "      <td>1690</td>\n",
              "      <td>7639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5631500400</td>\n",
              "      <td>2/25/2015</td>\n",
              "      <td>180000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>770</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>1933</td>\n",
              "      <td>0</td>\n",
              "      <td>98028</td>\n",
              "      <td>47.7379</td>\n",
              "      <td>-122.233</td>\n",
              "      <td>2720</td>\n",
              "      <td>8062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2487200875</td>\n",
              "      <td>12/9/2014</td>\n",
              "      <td>604000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1960</td>\n",
              "      <td>5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1050</td>\n",
              "      <td>910</td>\n",
              "      <td>1965</td>\n",
              "      <td>0</td>\n",
              "      <td>98136</td>\n",
              "      <td>47.5208</td>\n",
              "      <td>-122.393</td>\n",
              "      <td>1360</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1954400510</td>\n",
              "      <td>2/18/2015</td>\n",
              "      <td>510000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1680</td>\n",
              "      <td>8080</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1680</td>\n",
              "      <td>0</td>\n",
              "      <td>1987</td>\n",
              "      <td>0</td>\n",
              "      <td>98074</td>\n",
              "      <td>47.6168</td>\n",
              "      <td>-122.045</td>\n",
              "      <td>1800</td>\n",
              "      <td>7503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07045bdd-8583-48e5-8def-32e07e80edca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07045bdd-8583-48e5-8def-32e07e80edca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07045bdd-8583-48e5-8def-32e07e80edca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           id        date     price  ...     long  sqft_living15  sqft_lot15\n",
              "0  7129300520  10/13/2014  221900.0  ... -122.257           1340        5650\n",
              "1  6414100192   12/9/2014  538000.0  ... -122.319           1690        7639\n",
              "2  5631500400   2/25/2015  180000.0  ... -122.233           2720        8062\n",
              "3  2487200875   12/9/2014  604000.0  ... -122.393           1360        5000\n",
              "4  1954400510   2/18/2015  510000.0  ... -122.045           1800        7503\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjYRdUDhYllR"
      },
      "source": [
        "df['date']=pd.to_datetime(df['date']) # this is done to make date a timestamp object rather than a string object"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction "
      ],
      "metadata": {
        "id": "AMba-jxqrjDt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzDHOjl3YsTJ"
      },
      "source": [
        "df['year']=df['date'].apply(lambda date: date.year)   \n",
        "# The above steps is done for feature extraction of the year and month from the year object\n",
        "df['month']=df['date'].apply(lambda date: date.month)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping unnecessary columns( As they won't influence the prediction)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wcFaUeT7roPU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msSGh6s6ZW6r"
      },
      "source": [
        "X=df.drop('id',axis='columns')\n",
        "# This and the following 3 steps are done so that unnessary columns are dropped"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NNAwxXKZYhq"
      },
      "source": [
        "X=X.drop('date',axis='columns')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqTHrWSLaflC"
      },
      "source": [
        "X=X.drop('zipcode',axis=1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g0mctf9avLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3e4a6a-48dc-44fa-e105-89c65382922d"
      },
      "source": [
        "X=X.drop('price',axis=1)\n",
        "X=X.values\n",
        "X # Since a tensor flow model takes only matrix, the dataframe is converted to np array in this step"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.000e+00, 1.000e+00, 1.180e+03, ..., 5.650e+03, 2.014e+03,\n",
              "        1.000e+01],\n",
              "       [3.000e+00, 2.250e+00, 2.570e+03, ..., 7.639e+03, 2.014e+03,\n",
              "        1.200e+01],\n",
              "       [2.000e+00, 1.000e+00, 7.700e+02, ..., 8.062e+03, 2.015e+03,\n",
              "        2.000e+00],\n",
              "       ...,\n",
              "       [2.000e+00, 7.500e-01, 1.020e+03, ..., 2.007e+03, 2.014e+03,\n",
              "        6.000e+00],\n",
              "       [3.000e+00, 2.500e+00, 1.600e+03, ..., 1.287e+03, 2.015e+03,\n",
              "        1.000e+00],\n",
              "       [2.000e+00, 7.500e-01, 1.020e+03, ..., 1.357e+03, 2.014e+03,\n",
              "        1.000e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIoRz9bgawcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0113471-2ae5-472d-e62b-f682b7e848c3"
      },
      "source": [
        "y=df['price'].values\n",
        "y # y values of the dataframe is converted to np array\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([221900., 538000., 180000., ..., 402101., 400000., 325000.])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into training and test data"
      ],
      "metadata": {
        "id": "Lt7XZGmIr-gu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9_Ejy-xaxXS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)  \n",
        "# This is famous train test split"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ijiQMqbihV",
        "outputId": "f2178357-83af-47bd-e3ea-dd5b08493639"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.0000e+00, 1.0000e+00, 2.2900e+03, ..., 6.3000e+03, 2.0140e+03,\n",
              "        9.0000e+00],\n",
              "       [3.0000e+00, 2.2500e+00, 1.6300e+03, ..., 3.1310e+03, 2.0140e+03,\n",
              "        6.0000e+00],\n",
              "       [4.0000e+00, 3.5000e+00, 2.8500e+03, ..., 5.7080e+03, 2.0150e+03,\n",
              "        5.0000e+00],\n",
              "       ...,\n",
              "       [2.0000e+00, 1.7500e+00, 1.8000e+03, ..., 7.4000e+03, 2.0150e+03,\n",
              "        4.0000e+00],\n",
              "       [3.0000e+00, 2.0000e+00, 1.5000e+03, ..., 1.4013e+04, 2.0150e+03,\n",
              "        4.0000e+00],\n",
              "       [2.0000e+00, 1.0000e+00, 1.1600e+03, ..., 5.0760e+03, 2.0150e+03,\n",
              "        1.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalising the data between 0 and 1 (So that learning can be sped up)"
      ],
      "metadata": {
        "id": "q-9HrIjwsGkM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvuYvFoeb3HP"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler() \n",
        "# we would train our model on normalised data between 0 and 1 and that is why this is done"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5ZBbkGcFAM"
      },
      "source": [
        "X_train=scaler.fit_transform(X_train)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSVTpIfOcLIs"
      },
      "source": [
        "X_test=scaler.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desZ327ucR8h"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model=Sequential() #model reference is created"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a model with 5 layers and rectified linear unit activation function"
      ],
      "metadata": {
        "id": "lzKyIrD5sXAU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHqSqB3dgzoZ"
      },
      "source": [
        "model.add(Dense(19,activation='linear'))\n",
        "model.add(Dense(30,activation='linear'))\n",
        "model.add(Dense(30,activation='linear'))\n",
        "model.add(Dense(30,activation='relu'))   # model is created with no of layer and optimizer defined\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse') "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and fitting the model on the data "
      ],
      "metadata": {
        "id": "bEDDdd1csrHp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urvJ-SWYhwz5",
        "outputId": "8254bdf9-4c5b-4951-93e8-a0eff4179ab3"
      },
      "source": [
        "model.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=800) # this is done so that loss function on test data is evaluated all along"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "473/473 [==============================] - 2s 3ms/step - loss: 393676.5625 - val_loss: 180200.7969\n",
            "Epoch 2/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 179858.3594 - val_loss: 169832.2031\n",
            "Epoch 3/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 168702.3125 - val_loss: 158307.2656\n",
            "Epoch 4/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 155801.5938 - val_loss: 143583.2188\n",
            "Epoch 5/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 140922.8438 - val_loss: 129595.1875\n",
            "Epoch 6/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 130878.3516 - val_loss: 124489.7891\n",
            "Epoch 7/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 127074.2266 - val_loss: 122380.8516\n",
            "Epoch 8/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 125188.8516 - val_loss: 120603.2266\n",
            "Epoch 9/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 124012.3359 - val_loss: 119506.7969\n",
            "Epoch 10/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 123086.4141 - val_loss: 118705.6328\n",
            "Epoch 11/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 122326.3359 - val_loss: 118031.0391\n",
            "Epoch 12/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 121844.7500 - val_loss: 117526.8750\n",
            "Epoch 13/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 121406.0391 - val_loss: 117422.9297\n",
            "Epoch 14/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 121090.1406 - val_loss: 116759.0781\n",
            "Epoch 15/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 120702.1719 - val_loss: 116750.8516\n",
            "Epoch 16/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 120554.9531 - val_loss: 116641.7109\n",
            "Epoch 17/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 120183.0000 - val_loss: 116080.8672\n",
            "Epoch 18/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 120015.6484 - val_loss: 116515.6953\n",
            "Epoch 19/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 119843.5859 - val_loss: 116489.7578\n",
            "Epoch 20/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 119732.4453 - val_loss: 115768.2891\n",
            "Epoch 21/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119471.0156 - val_loss: 115294.5703\n",
            "Epoch 22/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119447.6016 - val_loss: 115197.8047\n",
            "Epoch 23/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119304.6094 - val_loss: 115064.4922\n",
            "Epoch 24/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119252.4219 - val_loss: 114956.4844\n",
            "Epoch 25/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119133.1641 - val_loss: 115061.2266\n",
            "Epoch 26/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119047.3125 - val_loss: 115421.3359\n",
            "Epoch 27/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118898.8750 - val_loss: 114791.2188\n",
            "Epoch 28/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 119023.7188 - val_loss: 114673.4062\n",
            "Epoch 29/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118769.3672 - val_loss: 115182.4531\n",
            "Epoch 30/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118729.2344 - val_loss: 115277.5625\n",
            "Epoch 31/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118666.7109 - val_loss: 114702.0938\n",
            "Epoch 32/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118596.5547 - val_loss: 114890.4375\n",
            "Epoch 33/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118630.4688 - val_loss: 114428.4766\n",
            "Epoch 34/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118549.8047 - val_loss: 114597.8359\n",
            "Epoch 35/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118500.2734 - val_loss: 114434.8750\n",
            "Epoch 36/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118462.5000 - val_loss: 114621.8281\n",
            "Epoch 37/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118466.3984 - val_loss: 114378.6406\n",
            "Epoch 38/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118316.5938 - val_loss: 114304.0859\n",
            "Epoch 39/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118359.2500 - val_loss: 114271.6953\n",
            "Epoch 40/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118396.7266 - val_loss: 114261.6875\n",
            "Epoch 41/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118382.1562 - val_loss: 114642.0859\n",
            "Epoch 42/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118367.8750 - val_loss: 114356.7734\n",
            "Epoch 43/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118289.4141 - val_loss: 115229.7891\n",
            "Epoch 44/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118346.1250 - val_loss: 114536.4922\n",
            "Epoch 45/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118240.1172 - val_loss: 114150.5938\n",
            "Epoch 46/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118195.1953 - val_loss: 114241.8359\n",
            "Epoch 47/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118196.2266 - val_loss: 114081.4297\n",
            "Epoch 48/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118251.8594 - val_loss: 114770.5391\n",
            "Epoch 49/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118185.7109 - val_loss: 114553.2344\n",
            "Epoch 50/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118223.6953 - val_loss: 114234.4609\n",
            "Epoch 51/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118209.8984 - val_loss: 114017.9922\n",
            "Epoch 52/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118194.1875 - val_loss: 114138.5391\n",
            "Epoch 53/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118107.1484 - val_loss: 114024.0781\n",
            "Epoch 54/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118257.0703 - val_loss: 114062.5547\n",
            "Epoch 55/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118174.9531 - val_loss: 114041.5391\n",
            "Epoch 56/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118090.2891 - val_loss: 114080.4453\n",
            "Epoch 57/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118138.2109 - val_loss: 114102.6484\n",
            "Epoch 58/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118145.3984 - val_loss: 114623.8203\n",
            "Epoch 59/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.8906 - val_loss: 114715.1016\n",
            "Epoch 60/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118078.6719 - val_loss: 114137.6172\n",
            "Epoch 61/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118127.7812 - val_loss: 114077.4062\n",
            "Epoch 62/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118164.9219 - val_loss: 114249.5781\n",
            "Epoch 63/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118041.7188 - val_loss: 114073.0859\n",
            "Epoch 64/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118096.5781 - val_loss: 113931.0938\n",
            "Epoch 65/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118126.6797 - val_loss: 114308.6328\n",
            "Epoch 66/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118074.0312 - val_loss: 114048.7812\n",
            "Epoch 67/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118072.7734 - val_loss: 114008.7734\n",
            "Epoch 68/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118014.5859 - val_loss: 113943.1328\n",
            "Epoch 69/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118061.8203 - val_loss: 114011.3281\n",
            "Epoch 70/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118133.6875 - val_loss: 113933.4219\n",
            "Epoch 71/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118209.7656 - val_loss: 113959.5234\n",
            "Epoch 72/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118137.4219 - val_loss: 114420.6484\n",
            "Epoch 73/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118064.1875 - val_loss: 113968.4453\n",
            "Epoch 74/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118080.1094 - val_loss: 114001.2578\n",
            "Epoch 75/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118008.2656 - val_loss: 114119.4844\n",
            "Epoch 76/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118066.3828 - val_loss: 113959.2500\n",
            "Epoch 77/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118059.4219 - val_loss: 114103.6172\n",
            "Epoch 78/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118075.8281 - val_loss: 114185.9375\n",
            "Epoch 79/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118108.7969 - val_loss: 113883.8203\n",
            "Epoch 80/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118070.3750 - val_loss: 114090.1719\n",
            "Epoch 81/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118141.4141 - val_loss: 113925.3203\n",
            "Epoch 82/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118045.6250 - val_loss: 113912.9766\n",
            "Epoch 83/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118066.7578 - val_loss: 113957.2578\n",
            "Epoch 84/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118023.4453 - val_loss: 114033.7344\n",
            "Epoch 85/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118203.1172 - val_loss: 113950.6094\n",
            "Epoch 86/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118045.6406 - val_loss: 113970.6953\n",
            "Epoch 87/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118049.8828 - val_loss: 114179.5938\n",
            "Epoch 88/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118103.2422 - val_loss: 114448.1484\n",
            "Epoch 89/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118094.3047 - val_loss: 114011.4766\n",
            "Epoch 90/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118057.1562 - val_loss: 113885.9922\n",
            "Epoch 91/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118113.6562 - val_loss: 114038.6484\n",
            "Epoch 92/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118029.8203 - val_loss: 114114.0547\n",
            "Epoch 93/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118092.5469 - val_loss: 113899.6328\n",
            "Epoch 94/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118100.9375 - val_loss: 113930.1719\n",
            "Epoch 95/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118028.9766 - val_loss: 113931.6719\n",
            "Epoch 96/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118170.1250 - val_loss: 114682.8125\n",
            "Epoch 97/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118080.4844 - val_loss: 113976.8672\n",
            "Epoch 98/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118035.3203 - val_loss: 114144.1406\n",
            "Epoch 99/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118160.8984 - val_loss: 113941.0781\n",
            "Epoch 100/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117982.0781 - val_loss: 113968.5703\n",
            "Epoch 101/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118110.2969 - val_loss: 114060.9219\n",
            "Epoch 102/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118048.6172 - val_loss: 113938.6562\n",
            "Epoch 103/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118085.0078 - val_loss: 114008.7812\n",
            "Epoch 104/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117982.5547 - val_loss: 114031.5859\n",
            "Epoch 105/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 118114.7109 - val_loss: 113948.2109\n",
            "Epoch 106/400\n",
            "473/473 [==============================] - 2s 4ms/step - loss: 118019.0547 - val_loss: 113876.8125\n",
            "Epoch 107/400\n",
            "473/473 [==============================] - 2s 5ms/step - loss: 118103.0859 - val_loss: 114087.9922\n",
            "Epoch 108/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118102.5547 - val_loss: 113936.9609\n",
            "Epoch 109/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118072.9766 - val_loss: 114761.5234\n",
            "Epoch 110/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118058.4141 - val_loss: 113943.2812\n",
            "Epoch 111/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118038.2656 - val_loss: 114079.6875\n",
            "Epoch 112/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118068.0156 - val_loss: 113933.5000\n",
            "Epoch 113/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118072.8984 - val_loss: 113873.3906\n",
            "Epoch 114/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118084.2812 - val_loss: 114188.1875\n",
            "Epoch 115/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118025.6562 - val_loss: 113949.0000\n",
            "Epoch 116/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118032.0000 - val_loss: 115538.7656\n",
            "Epoch 117/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118051.0547 - val_loss: 113962.5938\n",
            "Epoch 118/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118143.2969 - val_loss: 114241.5703\n",
            "Epoch 119/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118098.2031 - val_loss: 113949.2344\n",
            "Epoch 120/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117976.5859 - val_loss: 114083.4297\n",
            "Epoch 121/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118071.4844 - val_loss: 114104.1406\n",
            "Epoch 122/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118053.4688 - val_loss: 114061.8750\n",
            "Epoch 123/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118084.3203 - val_loss: 114125.7812\n",
            "Epoch 124/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118042.8828 - val_loss: 114117.9141\n",
            "Epoch 125/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118035.1484 - val_loss: 114059.6328\n",
            "Epoch 126/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118173.7734 - val_loss: 114143.8359\n",
            "Epoch 127/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118071.6562 - val_loss: 114020.6875\n",
            "Epoch 128/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.9766 - val_loss: 113896.5859\n",
            "Epoch 129/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118017.2109 - val_loss: 113871.8438\n",
            "Epoch 130/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118198.3516 - val_loss: 113910.9922\n",
            "Epoch 131/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118038.3828 - val_loss: 114267.5469\n",
            "Epoch 132/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.8516 - val_loss: 114237.4297\n",
            "Epoch 133/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118091.5000 - val_loss: 113919.1484\n",
            "Epoch 134/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118097.6328 - val_loss: 113909.3438\n",
            "Epoch 135/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118060.5781 - val_loss: 114257.6484\n",
            "Epoch 136/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118215.3047 - val_loss: 114055.7891\n",
            "Epoch 137/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118098.3672 - val_loss: 113935.9531\n",
            "Epoch 138/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118143.8906 - val_loss: 113985.1328\n",
            "Epoch 139/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118072.5000 - val_loss: 114175.2500\n",
            "Epoch 140/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117990.2109 - val_loss: 113859.9688\n",
            "Epoch 141/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118068.8438 - val_loss: 113939.7656\n",
            "Epoch 142/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118128.9219 - val_loss: 114052.5234\n",
            "Epoch 143/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118150.4375 - val_loss: 113900.7188\n",
            "Epoch 144/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118037.5859 - val_loss: 113968.3047\n",
            "Epoch 145/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118010.9766 - val_loss: 113998.1250\n",
            "Epoch 146/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118052.6641 - val_loss: 114017.0938\n",
            "Epoch 147/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118127.4922 - val_loss: 113959.5391\n",
            "Epoch 148/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118030.1719 - val_loss: 113884.6250\n",
            "Epoch 149/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118053.6953 - val_loss: 113895.6641\n",
            "Epoch 150/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118101.4062 - val_loss: 114000.1250\n",
            "Epoch 151/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118064.2188 - val_loss: 113937.0859\n",
            "Epoch 152/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.3047 - val_loss: 114160.9453\n",
            "Epoch 153/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118117.3750 - val_loss: 114204.5547\n",
            "Epoch 154/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118089.9844 - val_loss: 113881.6719\n",
            "Epoch 155/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118070.2578 - val_loss: 114071.1875\n",
            "Epoch 156/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118050.5391 - val_loss: 113948.5312\n",
            "Epoch 157/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118062.7734 - val_loss: 114174.8281\n",
            "Epoch 158/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.3984 - val_loss: 113957.0547\n",
            "Epoch 159/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118049.9453 - val_loss: 114453.7969\n",
            "Epoch 160/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118070.6953 - val_loss: 113859.8906\n",
            "Epoch 161/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118005.9922 - val_loss: 114284.4375\n",
            "Epoch 162/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118137.7109 - val_loss: 113851.5938\n",
            "Epoch 163/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118063.3828 - val_loss: 113978.7734\n",
            "Epoch 164/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118061.6562 - val_loss: 114245.0234\n",
            "Epoch 165/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118111.6172 - val_loss: 114001.8828\n",
            "Epoch 166/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118074.5312 - val_loss: 113878.6484\n",
            "Epoch 167/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118056.2891 - val_loss: 113866.3516\n",
            "Epoch 168/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118101.7734 - val_loss: 113975.6875\n",
            "Epoch 169/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118089.2500 - val_loss: 113965.1094\n",
            "Epoch 170/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118103.5703 - val_loss: 114164.1094\n",
            "Epoch 171/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118084.1484 - val_loss: 114592.5312\n",
            "Epoch 172/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118041.7578 - val_loss: 113985.3516\n",
            "Epoch 173/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118044.6484 - val_loss: 114131.0938\n",
            "Epoch 174/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118037.7422 - val_loss: 113875.0312\n",
            "Epoch 175/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118015.8984 - val_loss: 114013.0156\n",
            "Epoch 176/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118067.0156 - val_loss: 113904.9688\n",
            "Epoch 177/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118109.1172 - val_loss: 113924.9609\n",
            "Epoch 178/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118019.6016 - val_loss: 114225.4531\n",
            "Epoch 179/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117972.8359 - val_loss: 114712.1875\n",
            "Epoch 180/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118161.0156 - val_loss: 114004.2969\n",
            "Epoch 181/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117970.0781 - val_loss: 114001.3906\n",
            "Epoch 182/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118090.8516 - val_loss: 113972.9766\n",
            "Epoch 183/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118040.3672 - val_loss: 113895.5938\n",
            "Epoch 184/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118044.8516 - val_loss: 113879.4062\n",
            "Epoch 185/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118046.4219 - val_loss: 114229.4531\n",
            "Epoch 186/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118076.3828 - val_loss: 113946.9609\n",
            "Epoch 187/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118076.5938 - val_loss: 114224.9297\n",
            "Epoch 188/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118117.2422 - val_loss: 115126.5547\n",
            "Epoch 189/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118098.9219 - val_loss: 113953.6797\n",
            "Epoch 190/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118085.8438 - val_loss: 114134.5000\n",
            "Epoch 191/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118017.8750 - val_loss: 114291.6953\n",
            "Epoch 192/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118064.4219 - val_loss: 114041.2422\n",
            "Epoch 193/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118070.3438 - val_loss: 114149.3594\n",
            "Epoch 194/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118046.8359 - val_loss: 114073.8594\n",
            "Epoch 195/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118031.2344 - val_loss: 113943.3516\n",
            "Epoch 196/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117990.6875 - val_loss: 113999.5391\n",
            "Epoch 197/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118073.3516 - val_loss: 113884.3828\n",
            "Epoch 198/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118108.8984 - val_loss: 113953.0234\n",
            "Epoch 199/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118208.4844 - val_loss: 114628.3984\n",
            "Epoch 200/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117961.4844 - val_loss: 113903.6562\n",
            "Epoch 201/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117989.4453 - val_loss: 114244.1250\n",
            "Epoch 202/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118020.1172 - val_loss: 113929.6328\n",
            "Epoch 203/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118083.1875 - val_loss: 114567.1172\n",
            "Epoch 204/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118031.9453 - val_loss: 114143.2969\n",
            "Epoch 205/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118023.5078 - val_loss: 114185.8750\n",
            "Epoch 206/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118129.2266 - val_loss: 114211.4844\n",
            "Epoch 207/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118073.1562 - val_loss: 113946.4531\n",
            "Epoch 208/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118014.3438 - val_loss: 113936.4453\n",
            "Epoch 209/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118063.0547 - val_loss: 114026.6797\n",
            "Epoch 210/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118043.7266 - val_loss: 114304.2734\n",
            "Epoch 211/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118086.5391 - val_loss: 114080.6094\n",
            "Epoch 212/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118183.5781 - val_loss: 113844.3594\n",
            "Epoch 213/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118032.8125 - val_loss: 113869.8594\n",
            "Epoch 214/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117998.7500 - val_loss: 113857.3516\n",
            "Epoch 215/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118019.5469 - val_loss: 114033.1562\n",
            "Epoch 216/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118061.5234 - val_loss: 113970.6484\n",
            "Epoch 217/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118100.5938 - val_loss: 113919.4688\n",
            "Epoch 218/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118020.0391 - val_loss: 114111.3281\n",
            "Epoch 219/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118010.3984 - val_loss: 113968.8516\n",
            "Epoch 220/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118057.6484 - val_loss: 113920.3125\n",
            "Epoch 221/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118037.3203 - val_loss: 114263.9766\n",
            "Epoch 222/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118129.1328 - val_loss: 114248.3359\n",
            "Epoch 223/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118016.2422 - val_loss: 114356.0703\n",
            "Epoch 224/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118024.6328 - val_loss: 114490.3281\n",
            "Epoch 225/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118072.8594 - val_loss: 113999.2188\n",
            "Epoch 226/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118027.8203 - val_loss: 114750.5156\n",
            "Epoch 227/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118074.3203 - val_loss: 113972.0391\n",
            "Epoch 228/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117989.2656 - val_loss: 114466.9062\n",
            "Epoch 229/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118130.6953 - val_loss: 113976.6797\n",
            "Epoch 230/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118006.4297 - val_loss: 113956.3672\n",
            "Epoch 231/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118111.4297 - val_loss: 114457.6953\n",
            "Epoch 232/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118197.9844 - val_loss: 114502.1641\n",
            "Epoch 233/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118056.6016 - val_loss: 113939.4297\n",
            "Epoch 234/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118069.1641 - val_loss: 114568.2344\n",
            "Epoch 235/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118064.6953 - val_loss: 114136.9453\n",
            "Epoch 236/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118103.5312 - val_loss: 114038.9922\n",
            "Epoch 237/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118052.7500 - val_loss: 113976.0625\n",
            "Epoch 238/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118054.0078 - val_loss: 113917.2344\n",
            "Epoch 239/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118042.5078 - val_loss: 113910.6172\n",
            "Epoch 240/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118017.9375 - val_loss: 114140.0625\n",
            "Epoch 241/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117990.1719 - val_loss: 114164.0391\n",
            "Epoch 242/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118076.7969 - val_loss: 113942.4219\n",
            "Epoch 243/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118102.8359 - val_loss: 113885.0000\n",
            "Epoch 244/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118059.0312 - val_loss: 113834.8125\n",
            "Epoch 245/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118079.2109 - val_loss: 114394.6562\n",
            "Epoch 246/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118044.4609 - val_loss: 113884.5469\n",
            "Epoch 247/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118018.9219 - val_loss: 114462.1250\n",
            "Epoch 248/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118128.4766 - val_loss: 114063.5859\n",
            "Epoch 249/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118037.8281 - val_loss: 114061.2812\n",
            "Epoch 250/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118066.7109 - val_loss: 113913.5625\n",
            "Epoch 251/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117997.2188 - val_loss: 114028.7734\n",
            "Epoch 252/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118056.9219 - val_loss: 113903.7422\n",
            "Epoch 253/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118067.5000 - val_loss: 114333.3359\n",
            "Epoch 254/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118116.1719 - val_loss: 113886.9922\n",
            "Epoch 255/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118109.4219 - val_loss: 113911.8203\n",
            "Epoch 256/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118048.7344 - val_loss: 113984.6406\n",
            "Epoch 257/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118058.6562 - val_loss: 114209.2109\n",
            "Epoch 258/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118057.7109 - val_loss: 114407.0547\n",
            "Epoch 259/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118019.0156 - val_loss: 113914.0078\n",
            "Epoch 260/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118086.8828 - val_loss: 113957.0547\n",
            "Epoch 261/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118046.0938 - val_loss: 114129.4922\n",
            "Epoch 262/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.8516 - val_loss: 114110.2031\n",
            "Epoch 263/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118047.1016 - val_loss: 114219.0000\n",
            "Epoch 264/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118078.6797 - val_loss: 113918.4609\n",
            "Epoch 265/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118076.2422 - val_loss: 113930.8047\n",
            "Epoch 266/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118021.7422 - val_loss: 113891.3516\n",
            "Epoch 267/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118053.9688 - val_loss: 113927.6562\n",
            "Epoch 268/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118104.9062 - val_loss: 114170.4062\n",
            "Epoch 269/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118008.2422 - val_loss: 114163.6953\n",
            "Epoch 270/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118095.0859 - val_loss: 113917.6328\n",
            "Epoch 271/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118028.3359 - val_loss: 114164.2266\n",
            "Epoch 272/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118086.8438 - val_loss: 114113.3047\n",
            "Epoch 273/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118014.3750 - val_loss: 114567.3125\n",
            "Epoch 274/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118062.6953 - val_loss: 114205.6328\n",
            "Epoch 275/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118009.1875 - val_loss: 114539.8750\n",
            "Epoch 276/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117990.4766 - val_loss: 113940.3984\n",
            "Epoch 277/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118052.2891 - val_loss: 114287.3203\n",
            "Epoch 278/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118101.5000 - val_loss: 113871.2266\n",
            "Epoch 279/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118058.1562 - val_loss: 114089.4922\n",
            "Epoch 280/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118137.8281 - val_loss: 113992.9375\n",
            "Epoch 281/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118044.1953 - val_loss: 114224.8672\n",
            "Epoch 282/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118064.5156 - val_loss: 113881.5625\n",
            "Epoch 283/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118043.3516 - val_loss: 113878.3125\n",
            "Epoch 284/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118109.1875 - val_loss: 113900.1172\n",
            "Epoch 285/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118091.1641 - val_loss: 113977.7656\n",
            "Epoch 286/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118026.1484 - val_loss: 113957.6406\n",
            "Epoch 287/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118055.6250 - val_loss: 113987.2500\n",
            "Epoch 288/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118093.0625 - val_loss: 113947.0312\n",
            "Epoch 289/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118069.6562 - val_loss: 114119.8047\n",
            "Epoch 290/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118050.5078 - val_loss: 114555.6719\n",
            "Epoch 291/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118027.0703 - val_loss: 114210.4688\n",
            "Epoch 292/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118102.8125 - val_loss: 113991.1406\n",
            "Epoch 293/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118138.7734 - val_loss: 113884.9219\n",
            "Epoch 294/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117995.1172 - val_loss: 114086.8516\n",
            "Epoch 295/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118066.0938 - val_loss: 114180.2891\n",
            "Epoch 296/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118142.4062 - val_loss: 114146.7734\n",
            "Epoch 297/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118009.6562 - val_loss: 113913.8828\n",
            "Epoch 298/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118118.5156 - val_loss: 114513.1484\n",
            "Epoch 299/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118060.3281 - val_loss: 113972.9375\n",
            "Epoch 300/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118122.2891 - val_loss: 113907.3750\n",
            "Epoch 301/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118010.5703 - val_loss: 114249.1484\n",
            "Epoch 302/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118032.7656 - val_loss: 114326.6953\n",
            "Epoch 303/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118037.5391 - val_loss: 114045.1172\n",
            "Epoch 304/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118128.4688 - val_loss: 114012.1562\n",
            "Epoch 305/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118050.2188 - val_loss: 113928.8281\n",
            "Epoch 306/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118081.9766 - val_loss: 113943.6875\n",
            "Epoch 307/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118042.4609 - val_loss: 113858.2109\n",
            "Epoch 308/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118041.8047 - val_loss: 114188.6016\n",
            "Epoch 309/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118067.9453 - val_loss: 113878.6406\n",
            "Epoch 310/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118114.0234 - val_loss: 113928.6484\n",
            "Epoch 311/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118062.1719 - val_loss: 113888.0781\n",
            "Epoch 312/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118074.5000 - val_loss: 114360.2266\n",
            "Epoch 313/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118086.2266 - val_loss: 114053.0703\n",
            "Epoch 314/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117975.9219 - val_loss: 113990.4609\n",
            "Epoch 315/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118026.5156 - val_loss: 114015.5234\n",
            "Epoch 316/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118083.7422 - val_loss: 113871.9375\n",
            "Epoch 317/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118027.7578 - val_loss: 113939.6875\n",
            "Epoch 318/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118083.2188 - val_loss: 114130.0469\n",
            "Epoch 319/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118077.6172 - val_loss: 114105.8047\n",
            "Epoch 320/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118051.5469 - val_loss: 113898.7969\n",
            "Epoch 321/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117995.9062 - val_loss: 114042.6953\n",
            "Epoch 322/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118038.8984 - val_loss: 113935.4297\n",
            "Epoch 323/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118028.6484 - val_loss: 113892.7031\n",
            "Epoch 324/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118144.2891 - val_loss: 113964.2656\n",
            "Epoch 325/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118083.8594 - val_loss: 113979.7422\n",
            "Epoch 326/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118008.0156 - val_loss: 113867.2188\n",
            "Epoch 327/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118120.0781 - val_loss: 114003.9688\n",
            "Epoch 328/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118013.7031 - val_loss: 114057.8672\n",
            "Epoch 329/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118035.8750 - val_loss: 113939.2109\n",
            "Epoch 330/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118051.4219 - val_loss: 113920.3359\n",
            "Epoch 331/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118126.5625 - val_loss: 113939.5625\n",
            "Epoch 332/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118046.1406 - val_loss: 114011.1016\n",
            "Epoch 333/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118076.2344 - val_loss: 113999.2109\n",
            "Epoch 334/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118066.5547 - val_loss: 113960.4453\n",
            "Epoch 335/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118092.2578 - val_loss: 114777.1328\n",
            "Epoch 336/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118152.7344 - val_loss: 114114.2188\n",
            "Epoch 337/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118070.4141 - val_loss: 113934.0469\n",
            "Epoch 338/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118023.3203 - val_loss: 114022.6797\n",
            "Epoch 339/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118065.0312 - val_loss: 113917.8984\n",
            "Epoch 340/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118092.7031 - val_loss: 113994.2422\n",
            "Epoch 341/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118013.5625 - val_loss: 114558.9609\n",
            "Epoch 342/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118089.3672 - val_loss: 114198.8047\n",
            "Epoch 343/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118145.6484 - val_loss: 113901.5469\n",
            "Epoch 344/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118126.6094 - val_loss: 114082.3672\n",
            "Epoch 345/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118048.6094 - val_loss: 114012.6484\n",
            "Epoch 346/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118112.1484 - val_loss: 114604.2266\n",
            "Epoch 347/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118088.9844 - val_loss: 114098.9531\n",
            "Epoch 348/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118011.7266 - val_loss: 114286.9766\n",
            "Epoch 349/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118090.9297 - val_loss: 114021.6016\n",
            "Epoch 350/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118042.1875 - val_loss: 113947.3750\n",
            "Epoch 351/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118102.7031 - val_loss: 114304.1406\n",
            "Epoch 352/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118188.3125 - val_loss: 114123.2500\n",
            "Epoch 353/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118158.2500 - val_loss: 113906.8672\n",
            "Epoch 354/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118085.0625 - val_loss: 113972.6094\n",
            "Epoch 355/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118043.5234 - val_loss: 113980.7422\n",
            "Epoch 356/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118102.4219 - val_loss: 114009.4688\n",
            "Epoch 357/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118133.3750 - val_loss: 114068.7031\n",
            "Epoch 358/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118009.3125 - val_loss: 114741.4453\n",
            "Epoch 359/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118097.7891 - val_loss: 114601.6094\n",
            "Epoch 360/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118040.0469 - val_loss: 114259.6250\n",
            "Epoch 361/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118091.2188 - val_loss: 114053.1562\n",
            "Epoch 362/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118045.1172 - val_loss: 114689.6172\n",
            "Epoch 363/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 117985.4688 - val_loss: 113859.6250\n",
            "Epoch 364/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118047.1797 - val_loss: 113969.6484\n",
            "Epoch 365/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118061.4531 - val_loss: 114208.2109\n",
            "Epoch 366/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118089.2266 - val_loss: 114108.4531\n",
            "Epoch 367/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118100.9219 - val_loss: 113921.9531\n",
            "Epoch 368/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118014.8203 - val_loss: 114621.2344\n",
            "Epoch 369/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118038.8359 - val_loss: 113902.0156\n",
            "Epoch 370/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118013.1250 - val_loss: 114162.0156\n",
            "Epoch 371/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118044.8359 - val_loss: 113940.1406\n",
            "Epoch 372/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118018.7500 - val_loss: 113939.4688\n",
            "Epoch 373/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118127.8750 - val_loss: 113951.4297\n",
            "Epoch 374/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118021.2812 - val_loss: 114013.2422\n",
            "Epoch 375/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118058.4766 - val_loss: 114087.6250\n",
            "Epoch 376/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118040.3047 - val_loss: 113978.1016\n",
            "Epoch 377/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118157.4141 - val_loss: 113913.3438\n",
            "Epoch 378/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 117950.9297 - val_loss: 114188.7422\n",
            "Epoch 379/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118037.6016 - val_loss: 114355.8438\n",
            "Epoch 380/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118048.1641 - val_loss: 114548.6484\n",
            "Epoch 381/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118160.1016 - val_loss: 113954.6641\n",
            "Epoch 382/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118104.8828 - val_loss: 113921.1328\n",
            "Epoch 383/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118046.8281 - val_loss: 113937.6562\n",
            "Epoch 384/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118044.5156 - val_loss: 114012.5000\n",
            "Epoch 385/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118043.7266 - val_loss: 114258.5859\n",
            "Epoch 386/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118109.8672 - val_loss: 113927.9375\n",
            "Epoch 387/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118051.6172 - val_loss: 113911.7266\n",
            "Epoch 388/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118088.3203 - val_loss: 113917.8359\n",
            "Epoch 389/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118071.1406 - val_loss: 113894.2891\n",
            "Epoch 390/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118028.5938 - val_loss: 114058.8438\n",
            "Epoch 391/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118027.7344 - val_loss: 114228.5547\n",
            "Epoch 392/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118160.2812 - val_loss: 113932.5859\n",
            "Epoch 393/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118094.0781 - val_loss: 114054.5156\n",
            "Epoch 394/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118064.6719 - val_loss: 113947.0781\n",
            "Epoch 395/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118082.1406 - val_loss: 114080.2188\n",
            "Epoch 396/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118055.7734 - val_loss: 113983.7031\n",
            "Epoch 397/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118039.5312 - val_loss: 113897.2031\n",
            "Epoch 398/400\n",
            "473/473 [==============================] - 1s 3ms/step - loss: 118028.0312 - val_loss: 113937.9375\n",
            "Epoch 399/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118170.0000 - val_loss: 114056.1484\n",
            "Epoch 400/400\n",
            "473/473 [==============================] - 1s 2ms/step - loss: 118009.6562 - val_loss: 114151.4297\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8103d503d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kouClxm8ib-f"
      },
      "source": [
        "losses=pd.DataFrame(model.history.history) # Gives history of all the losses # Checking the losses and comparing with the val_data loss we can check overfitting of the data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting training and test loss to check overfit or underfit"
      ],
      "metadata": {
        "id": "esbHC2dKsyYz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "U9HjSUNIldr8",
        "outputId": "70781fc9-436d-4d6c-c3e3-1e5001eadc93"
      },
      "source": [
        "plt.figure(figsize=(22,10))\n",
        "losses.plot()   # we could continue training, if there is increase in val_loss then it will be overfitting"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8100565b10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1584x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model "
      ],
      "metadata": {
        "id": "Vmh0gDfBs7Sg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkeDqu-WnTTp"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foq8re5jod94"
      },
      "source": [
        "predictions=model.predict(X_test)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fM3y1y_ohWl",
        "outputId": "7be14e74-be1c-40d6-c93c-ff32abe39af9"
      },
      "source": [
        "mean_absolute_error(predictions,y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114151.42459731868"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAn9GfDMo8Fv",
        "outputId": "ae96d23f-3a87-481f-81c8-ebbc46a21252"
      },
      "source": [
        "df['price'].mean() # Seeing the previous cell and comparing with the mean price of the house we get to know that prediction model is not upto to the standard"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "540296.5735055795"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9259OFDqDFj",
        "outputId": "778a0ce4-eed1-4d22-ac80-5f89c83507ad"
      },
      "source": [
        "explained_variance_score(y_test,predictions)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6596124021590883"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plot to check prdictions with actual labelled output"
      ],
      "metadata": {
        "id": "_tVqbRqut4_y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "AsvsjD3UqpO3",
        "outputId": "8712c19a-bf61-49d0-8680-60d613f65742"
      },
      "source": [
        "plt.scatter(y_test,predictions)\n",
        "plt.plot(y_test,y_test,'r')\n",
        "# This shows that the outliers have hurt the model predictions thats why prediction were off and we could observe that house below 3 Lac our predictions were very good"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8103bfd510>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyM3W5yAr-NM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72ea3fe-baff-4b17-f4ce-e22d19011a06"
      },
      "source": [
        "# prediction on new house\n",
        "single_house=df.drop('price',axis=1).iloc[0] # getting single row of your data andf predicting\n",
        "single_house=single_house.drop(labels=['id', 'zipcode','date'])\n",
        "single_house"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bedrooms               3\n",
              "bathrooms            1.0\n",
              "sqft_living         1180\n",
              "sqft_lot            5650\n",
              "floors               1.0\n",
              "waterfront             0\n",
              "view                   0\n",
              "condition              3\n",
              "grade                  7\n",
              "sqft_above          1180\n",
              "sqft_basement          0\n",
              "yr_built            1955\n",
              "yr_renovated           0\n",
              "lat              47.5112\n",
              "long            -122.257\n",
              "sqft_living15       1340\n",
              "sqft_lot15          5650\n",
              "year                2014\n",
              "month                 10\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B3t45thtLHM"
      },
      "source": [
        "#single_house=single_house.values . This is done to convert to numpy array as model takes in arrays only\n",
        "single_house1=scaler.transform(single_house.values.reshape(-1,19))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkG4dzB4tN88",
        "outputId": "e29ab4f3-fd73-4500-d405-d887394ba05d"
      },
      "source": [
        "model.predict(single_house1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[268033.4]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVWA9pup3JDf"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}